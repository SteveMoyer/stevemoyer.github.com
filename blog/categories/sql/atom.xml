<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: sql | Coding Libetartian]]></title>
  <link href="http://blog.stevemoyer.net/blog/categories/sql/atom.xml" rel="self"/>
  <link href="http://blog.stevemoyer.net/"/>
  <updated>2013-03-21T12:30:27-07:00</updated>
  <id>http://blog.stevemoyer.net/</id>
  <author>
    <name><![CDATA[Steve Moyer]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Breaking Down Unruly Databases: The Problem]]></title>
    <link href="http://blog.stevemoyer.net/2011/03/08/breaking-down-unruly-databases-problem.html"/>
    <updated>2011-03-08T00:05:00-08:00</updated>
    <id>http://blog.stevemoyer.net/2011/03/08/breaking-down-unruly-databases-problem</id>
    <content type="html"><![CDATA[<div class='post'>
In my <a href="http://blog.stevemoyer.net/2011/03/ndump-managing-development-and-test.html">last post</a> on <a href="https://github.com/SteveMoyer/nDump/wiki">nDump </a>I talked about a tool I've created to import and export data from MS SQL Server databases. &nbsp;In this post I want to lay out some of my motivation for doing so.<br />
<br />
Often when starting at a new client we arrive to find a legacy database/databases which seem nearly incomprehensible.  Development and CI often run against a shared databases which are backups of production.  The state of the data is in a constant flux. &nbsp;If someone desires to run a local database, restoring at least a few gigs of database backups is required.  One reason this is necessary is that the database(s) can't easily be recreated from scripts.  This is relatively easy to get around by breaking creation scripts into parts and employing tools such as <a href="http://dbdeploy.com/">DBDeploy</a> to run them in order.<br />
<br />
Once the problem of creating an empty database is tackled, nobody seems to have the requisite knowledge to populate it with the basic data necessary to run the application.  There are many problems that spring from a lack of the ability to create minimal data sets: <br />
<div><ol><li>Returning to a known data set is time consuming, requiring  restoring a large backup.</li>
<li>The development and functional test data sets can't be easily version-ed.  While you can check in a backup, there is no reasonable way to diff it against a previous version.</li>
<li>Functional tests are often brittle as they rely upon the state of the data which tends to be rather inconsistent.</li>
<li>Finding data requires searching through a large data set.</li>
<li>Deciphering incremental changes to the state of the data  is no small challenge.</li>
</ol><div>On past projects, the teams have stored development and test data sets as groups of csv or XML files.  Both of these files can be easily version-ed, diff-ed, and modified using excel or open office.  I prefer csv files as they are more concise and contain a lot less noise.&nbsp; Once you have these file sets and the requisite knowledge of the database structure it's relatively pleasant to work with them to maintain known state(s) of the data. By running the database build and load in a CI build we can continually verify that we know how to build a database. Using the populated database we can confirm that our functional tests of the application(s) are still working against a consistent data set. </div></div></div>



]]></content>
  </entry>
  
</feed>
